{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3523a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tempfile\n",
    "import logging\n",
    "from enum import Enum\n",
    "from functools import lru_cache as cache\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sheetsage.align import create_beat_to_time_fn\n",
    "from sheetsage.assets import retrieve_asset\n",
    "from sheetsage.beat_track import madmom\n",
    "from sheetsage.modules import EncOnlyTransducer, TransformerEncoder, IdentityEncoder\n",
    "from sheetsage.representations import Handcrafted, Jukebox\n",
    "from sheetsage.utils import decode_audio, retrieve_audio_bytes, get_approximate_audio_length\n",
    "\n",
    "class InputFeats(Enum):\n",
    "    HANDCRAFTED = 0\n",
    "    JUKEBOX = 1\n",
    "\n",
    "\n",
    "class Task(Enum):\n",
    "    MELODY = 0\n",
    "    HARMONY = 1\n",
    "\n",
    "\n",
    "class Model(Enum):\n",
    "    LINEAR = 0\n",
    "    TRANSFORMER = 1\n",
    "    \n",
    "\n",
    "class OutputModality(Enum):\n",
    "    MELODY_MIDI = 0\n",
    "    LEAD_SHEET = 1\n",
    "\n",
    "\n",
    "_INPUT_TO_FRAME_RATE = {\n",
    "    InputFeats.HANDCRAFTED: 16000 / 512,\n",
    "    InputFeats.JUKEBOX: 44100 / 128,\n",
    "}\n",
    "_INPUT_TO_DIM = {\n",
    "    InputFeats.HANDCRAFTED: 229,\n",
    "    InputFeats.JUKEBOX: 4800,\n",
    "}\n",
    "_JUKEBOX_CHUNK_DURATION_EDGE = 23.75\n",
    "_TERTIARIES_PER_BEAT = 4\n",
    "_HARMONY_FAMILIES = [\"\", \"m\", \"m7\", \"7\", \"maj7\", \"sus\", \"dim\", \"aug\"]\n",
    "_TASK_TO_VOCAB = {\n",
    "    Task.MELODY: [None] + list(range(21, 109)),\n",
    "    Task.HARMONY: [None] + list(itertools.product(list(range(12)), _HARMONY_FAMILIES)),\n",
    "}\n",
    "_MAX_TERTIARIES_PER_CHUNK = 384\n",
    "\n",
    "@cache()\n",
    "def _init_extractor(input_feats):\n",
    "    if input_feats == InputFeats.HANDCRAFTED:\n",
    "        extractor = Handcrafted()\n",
    "    elif input_feats == InputFeats.JUKEBOX:\n",
    "        extractor = Jukebox()\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return extractor\n",
    "\n",
    "\n",
    "@cache()\n",
    "def _init_model(task, input_feats, model):\n",
    "    if model == Model.LINEAR:\n",
    "        # NOTE: Just need to catalogue these configs / weights\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    asset_prefix = f\"SHEETSAGE_V02_{input_feats.name}_{task.name}\"\n",
    "    with open(retrieve_asset(f\"{asset_prefix}_CFG\"), 'r') as f:\n",
    "        cfg = json.load(f)\n",
    "    assert cfg['src_max_len'] == _MAX_TERTIARIES_PER_CHUNK\n",
    "    \n",
    "    src_dim = _INPUT_TO_DIM[input_feats]\n",
    "    output_dim = len(_TASK_TO_VOCAB[task])\n",
    "    \n",
    "    if cfg[\"model\"] == \"probe\":\n",
    "        model = EncOnlyTransducer(\n",
    "            output_dim,\n",
    "            src_emb_mode=\"identity\",\n",
    "            src_vocab_size=None,\n",
    "            src_dim=src_dim,\n",
    "            src_emb_dim=None,\n",
    "            src_pos_emb=False,\n",
    "            src_dropout_p=0.0,\n",
    "            enc_cls=IdentityEncoder,\n",
    "            enc_kwargs={},\n",
    "        )\n",
    "    elif cfg[\"model\"] == \"transformer\":\n",
    "        model = EncOnlyTransducer(\n",
    "            output_dim,\n",
    "            src_emb_mode=\"project\",\n",
    "            src_vocab_size=None,\n",
    "            src_dim=src_dim,\n",
    "            src_emb_dim=512,\n",
    "            src_pos_emb=\"pos_emb\" in cfg[\"hacks\"],\n",
    "            src_dropout_p=0.1,\n",
    "            enc_cls=TransformerEncoder,\n",
    "            enc_kwargs={\n",
    "                \"model_dim\": 512,\n",
    "                \"num_heads\": 8,\n",
    "                \"num_layers\": 4 if \"4layers\" in cfg[\"hacks\"] else 6,\n",
    "                \"feedforward_dim\": 2048,\n",
    "                \"dropout_p\": 0.1,\n",
    "            },\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError()\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(\n",
    "        retrieve_asset(f\"{asset_prefix}_MODEL\"), map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "    \n",
    "def _closest_idx(x, l):\n",
    "    assert len(l) > 0\n",
    "    return np.argmin([abs(li - x) for li in l])\n",
    "\n",
    "\n",
    "def sheetsage(\n",
    "    audio_path_or_bytes_url,\n",
    "    segment_start_hint=None,\n",
    "    segment_end_hint=None,\n",
    "    segment_hints_are_downbeats=False,\n",
    "    measures_per_segment=8,\n",
    "    input_feats=InputFeats.JUKEBOX,\n",
    "    output_modality=OutputModality.LEAD_SHEET,\n",
    "    beat_detection_padding=15.0,\n",
    "    beats_per_measure_hint=None):\n",
    "    \"\"\"Main driver function for Sheet Sage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    audio_path_or_bytes_url : :class:`pathlib.Path`, bytes, or str\n",
    "       The filepath, raw bytes, or string URL of the audio file.\n",
    "    segment_start_hint : float or None\n",
    "       Pass\n",
    "    segment_end_hint : float or None\n",
    "       Pass\n",
    "    input_feats : :class:`Input`\n",
    "    output_modality : :class:`Output`\n",
    "    measures_per_segment : int\n",
    "    beat_detection_padding : float\n",
    "    beats_per_measure_hint : int or None\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    # Check arguments\n",
    "    if segment_start_hint is not None:\n",
    "        if segment_start_hint < 0:\n",
    "            raise ValueError('Segment start hint must be non-negative')\n",
    "    if segment_end_hint is not None:\n",
    "        if segment_end_hint < 0:\n",
    "            raise ValueError('Segment end hint must be non-negative')\n",
    "        if segment_start_hint is None:\n",
    "            raise ValueError('Must specify a start hint with end hint')\n",
    "    if segment_start_hint is not None and segment_end_hint is not None:\n",
    "        if segment_end_hint <= segment_start_hint:\n",
    "            raise ValueError('Segment end before segment start')\n",
    "    if measures_per_segment <= 0:\n",
    "        raise ValueError('Invalid measures per segment specified')\n",
    "    if measures_per_segment > 24:\n",
    "        raise ValueError('Sheet Sage can only transcribe 24 measures per segment')\n",
    "    if beats_per_measure_hint is not None and beats_per_measure_hint not in [3, 4]:\n",
    "        raise ValueError('Currently, Sheet Sage only supports 4/4 and 3/4 time signatures')\n",
    "    \n",
    "    # Download audio if URL specified\n",
    "    audio_path_or_bytes = audio_path_or_bytes_url\n",
    "    if isinstance(audio_path_or_bytes_url, str):\n",
    "        logging.info(f'Retrieving audio from {audio_path_or_bytes_url}')\n",
    "        audio_path_or_bytes = retrieve_audio_bytes(audio_path_or_bytes_url)\n",
    "    \n",
    "    # Run beat detection\n",
    "    logging.info('Detecting beats')\n",
    "    beat_detection_start = 0.0 if segment_start_hint is None else segment_start_hint\n",
    "    beat_detection_start = max(beat_detection_start - beat_detection_padding, 0.0)\n",
    "    beat_detection_end = None if segment_end_hint is None else segment_end_hint\n",
    "    beat_detection_end = None if beat_detection_end is None else beat_detection_end + beat_detection_padding\n",
    "    sr, audio = decode_audio(\n",
    "        audio_path_or_bytes,\n",
    "        offset=beat_detection_start,\n",
    "        duration=None if beat_detection_end is None else beat_detection_end - beat_detection_start)\n",
    "    first_downbeat_idx, beats_per_measure, beats = madmom(\n",
    "        sr,\n",
    "        audio,\n",
    "        beats_per_bar=beats_per_measure_hint if beats_per_measure_hint is not None else [3, 4])\n",
    "    if first_downbeat_idx is None or beats_per_measure is None or len(beats) == 0:\n",
    "        raise ValueError('Audio too short to detect time signature')\n",
    "    assert first_downbeat_idx >= 0 and first_downbeat_idx < beats_per_measure\n",
    "    assert beats_per_measure in [3, 4]\n",
    "    beats = [beat_detection_start + t for t in beats]\n",
    "    downbeats = [t for i, t in enumerate(beats) if i % beats_per_measure == first_downbeat_idx % beats_per_measure]\n",
    "    assert len(beats) > 0\n",
    "    assert len(downbeats) > 0\n",
    "    \n",
    "    # Convert beats into tertiary (sixteenth note) timestamps\n",
    "    # NOTE: Yes, this is super ugly, but sometimes you gotta do what works :shrug:\n",
    "    beat_to_time_fn = create_beat_to_time_fn(list(range(len(beats))), beats)\n",
    "    tertiaries_raw = np.arange(0, len(beats) - 1 + 1e-6, 1 / _TERTIARIES_PER_BEAT)\n",
    "    assert tertiaries_raw.shape[0] == (len(beats) - 1) * _TERTIARIES_PER_BEAT + 1\n",
    "    tertiaries = tertiaries_raw - (1 / _TERTIARIES_PER_BEAT) / 2\n",
    "    tertiary_times = beat_to_time_fn(tertiaries)\n",
    "    tertiary_times = np.maximum(tertiary_times, 0.0)\n",
    "    tertiary_times = np.minimum(tertiary_times, beats[-1])\n",
    "    tertiary_diff_frames = np.diff(tertiary_times) * _INPUT_TO_FRAME_RATE[input_feats]\n",
    "    if np.any(tertiary_diff_frames.astype(np.int64) == 0):\n",
    "        raise ValueError('Tempo too fast for beat-informed feature resampling')\n",
    "    \n",
    "    # Find first downbeat of the song from optional hint\n",
    "    if segment_start_hint is not None:\n",
    "        if segment_hints_are_downbeats:\n",
    "            first_downbeat = segment_start_hint\n",
    "        else:\n",
    "            first_downbeat = downbeats[_closest_idx(segment_start_hint, downbeats)]\n",
    "        first_downbeat_idx = _closest_idx(first_downbeat, beats)\n",
    "        downbeats = [t for i, t in enumerate(beats) if i % beats_per_measure == first_downbeat_idx % beats_per_measure]\n",
    "    \n",
    "    # Find last downbeat of the song from optional hint\n",
    "    if segment_end_hint is None:\n",
    "        last_downbeat = downbeats[-1]\n",
    "    else:\n",
    "        if segment_hints_are_downbeats:\n",
    "            last_downbeat = segment_end_hint\n",
    "        else:\n",
    "            last_downbeat = downbeats[_closest_idx(segment_end_hint, downbeats)]\n",
    "    last_downbeat_idx = _closest_idx(last_downbeat, beats)\n",
    "    \n",
    "    # Identify suitable chunks for running through transcription model\n",
    "    tertiary_chunks = []\n",
    "    beats_per_segment = beats_per_measure * measures_per_segment\n",
    "    tertiaries_per_segment = _TERTIARIES_PER_BEAT * beats_per_segment\n",
    "    for beat_idx in range(first_downbeat_idx, last_downbeat_idx, beats_per_segment):\n",
    "        tertiary_start_idx = beat_idx * _TERTIARIES_PER_BEAT\n",
    "        tertiary_end_idx = ((beat_idx + beats_per_segment) * _TERTIARIES_PER_BEAT) + 1\n",
    "        tertiary_end_idx = min(tertiary_end_idx, tertiary_times.shape[0])\n",
    "        duration = tertiary_times[tertiary_end_idx - 1] - tertiary_times[tertiary_start_idx]\n",
    "        assert duration > 0\n",
    "        if duration > _JUKEBOX_CHUNK_DURATION_EDGE:\n",
    "            raise NotImplementedError('Dynamic chunking not implemented. Try halving measures_per_segment.')\n",
    "        tertiary_chunks.append((tertiary_start_idx, tertiary_end_idx))\n",
    "\n",
    "    # Extract features\n",
    "    logging.info('Extracting feats')\n",
    "    extractor = _init_extractor(input_feats)\n",
    "    features = []\n",
    "    with tempfile.NamedTemporaryFile(\"wb\") as f:\n",
    "        if isinstance(audio_path_or_bytes, bytes):\n",
    "            f.write(audio_path_or_bytes)\n",
    "            f.flush()\n",
    "            audio_path = f.name\n",
    "        else:\n",
    "            audio_path = audio_path_or_bytes\n",
    "\n",
    "        for tertiary_start_idx, tertiary_end_idx in tertiary_chunks:\n",
    "            chunk_tertiaries = tertiary_times[tertiary_start_idx:tertiary_end_idx]\n",
    "            offset = chunk_tertiaries[0]\n",
    "            duration = chunk_tertiaries[-1] - offset\n",
    "            assert duration <= _JUKEBOX_CHUNK_DURATION_EDGE\n",
    "            fr, feats = extractor(\n",
    "                audio_path, offset=offset, duration=duration)\n",
    "            \n",
    "            beat_resampled = []\n",
    "            for i in range(chunk_tertiaries.shape[0] - 1):\n",
    "                s = int((chunk_tertiaries[i] - offset) * fr)\n",
    "                e = int((chunk_tertiaries[i + 1] - offset) * fr)\n",
    "                assert e > s\n",
    "                beat_resampled.append(np.mean(feats[s:e], axis=0, keepdims=True))\n",
    "            beat_resampled = np.concatenate(beat_resampled, axis=0)\n",
    "            \n",
    "            features.append(beat_resampled)\n",
    "    \n",
    "    # Transcribe chunks\n",
    "    logging.info('Transcribing')\n",
    "    melody_model = _init_model(Task.MELODY, input_feats, Model.TRANSFORMER)\n",
    "    melody_logits = []\n",
    "    if output_modality == OutputModality.LEAD_SHEET:\n",
    "        harmony_model = _init_model(Task.HARMONY, input_feats, Model.TRANSFORMER)\n",
    "        harmony_logits = []\n",
    "    device = torch.device('cpu')\n",
    "    with torch.no_grad():\n",
    "        for src in features:\n",
    "            src_len = src.shape[0]\n",
    "            src = np.pad(src, [(0, _MAX_TERTIARIES_PER_CHUNK - src_len), (0, 0)])\n",
    "            src = src[:, np.newaxis]\n",
    "            src = torch.tensor(src).float()\n",
    "            src_len = torch.tensor(src_len).long().view(-1)\n",
    "            src.to(device)\n",
    "            src_len.to(device)\n",
    "\n",
    "            chunk_melody_logits = melody_model(src, src_len, None, None)[: src_len.item(), 0]\n",
    "            melody_logits.append(chunk_melody_logits.cpu().numpy())\n",
    "            if output_modality == OutputModality.LEAD_SHEET:\n",
    "                chunk_harmony_logits = harmony_model(src, src_len, None, None)[: src_len.item(), 0]\n",
    "                harmony_logits.append(chunk_harmony_logits.cpu().numpy())\n",
    "    \n",
    "    melody_logits = np.concatenate(melody_logits, axis=0)\n",
    "    if output_modality == OutputModality.LEAD_SHEET:\n",
    "        harmony_logits = np.concatenate(harmony_logits, axis=0)\n",
    "    print(melody_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1011f6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4094821\n"
     ]
    }
   ],
   "source": [
    "from sheetsage.utils import decode_audio, retrieve_audio_bytes\n",
    "\n",
    "audio_bytes = retrieve_audio_bytes('https://www.youtube.com/watch?v=4NRXx6U8ABQ')\n",
    "print(len(audio_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f05f5756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 89)\n"
     ]
    }
   ],
   "source": [
    "sheetsage(\n",
    "    audio_bytes,\n",
    "    input_feats=InputFeats.HANDCRAFTED,\n",
    "    output_modality=OutputModality.MELODY_MIDI,\n",
    "    segment_start_hint=39.5,\n",
    "    segment_end_hint=50.5,\n",
    "    measures_per_segment=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
